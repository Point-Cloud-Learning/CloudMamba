common: {
  epoch: 400,
  num_point: 1024,
  experiment_dir: "Logs/",
  log_name: "main",
  step_per_update: 1,
  grad_norm_clip: 10,
  val_freq: 1,
}

optimizer: {
  type: AdamW,
  kwargs: {
    lr: 0.001,
    weight_decay: 0.05
  }
}

scheduler: {
  type: CosLR,
  kwargs: {
    epochs: 400,
    initial_epochs: 10,
    lr_min: 0.000001
  }
}

augmentation:
  - rotate_point_cloud
  - random_scale_point_cloud
  - shift_point_cloud
  - jitter_point_cloud
